# FSCrawler demo script

## Setup

Create a `.env` file. It must contain the following settings:

```sh
# FSCrawler settings 
FSCRAWLER_VERSION=2.10-SNAPSHOT
FSCRAWLER_DISTRIBUTION=~/.m2/repository/fr/pilato/elasticsearch/crawler/fscrawler-distribution

# Settings for fscrawler cloud instance
ELASTIC_VERSION=8.5.2
ELASTIC_USERNAME=elastic
ELASTIC_PASSWORD=changeme-im-not-safe
CLOUD_ID=<CLOUD_ID>
ELASTICSEARCH_URL=https://ENDPOINT.es.REGION.PROVIDER.elastic-cloud.com:9243
KIBANA_URL=https://ENDPOINT.kb.REGION.PROVIDER.elastic-cloud.com:9243
WORKPLACE_URL=https://ENDPOINT.ent.REGION.PROVIDER.elastic-cloud.com

# Apache Tika standalone demo
TIKA_VERSION=2.6.0
```

You might have to replace some based on your cloud deployment and some local settings:

* `FSCRAWLER_VERSION`: FSCrawler version.
* `FSCRAWLER_DISTRIBUTION`: FSCrawler distribution dir. It must contain a `FSCRAWLER_VERSION` dir which contains a file named `fscrawler-distribution-<FSCRAWLER_VERSION>.zip`. If you are using maven to build the project, you can use `~/.m2/repository/fr/pilato/elasticsearch/crawler/fscrawler-distribution` as the dir name.
* `ELASTIC_VERSION`: Elastic stack version.
* `ELASTIC_USERNAME`: you could create another user than `elastic` with reduced privileges (recommended).
* `ELASTIC_PASSWORD`: the user name password
* `CLOUD_ID`: check the cloud service console to get this.
* `ELASTICSEARCH_URL`: get the Elasticsearch endpoint URL from the cloud service console.
* `KIBANA_URL`: get the Kibana endpoint URL from the cloud service console.
* `WORKPLACE_URL`: get the Enterprise Search endpoint URL from the cloud service console.
* `TIKA_VERSION`: the version of the latest Tika release. Only helpful for demoing Tika standalone.

You will need to start a local Python webserver:

```sh
cd docs/full; python3 -m http.server --cgi 80
```

Start the setup:

```sh
./setup.sh
```

It will check that the needed services are available.

## Configuration

Run:

```sh
./configure.sh
```

This will remove all the data before the demo and generate all the needed jobs.
It will also give you some end instructions to follow.

Open Cloud and use the Kibana link of the deployment.

## Tika standalone demo

Run:

```sh
# Extracted text
cat docs/test/foo.txt | java -jar tika/tika-app-*.jar --text
# Metadata
cat docs/test/foo.txt | java -jar tika/tika-app-*.jar --metadata
# Metadata as JSON
cat docs/test/foo.txt | java -jar tika/tika-app-*.jar --json | jq
```

## Ingest Attachment Plugin

Run:

```sh
cat docs/test/foo.txt | base64
```

It gives:

```txt
VGhpcyBpcyBhIHNhbXBsZSBkb2N1bWVudCB0byBtYWtlIHN1cmUgRlNDcmF3bGVyCmlzIHdlbGwgY29ubmVjdGVkIHRvIEVsYXN0aWMgV29ya3BsYWNlIFNlYXJjaC4KCg==
```

Replace the BASE64 content and run it from the Dev Console.

Show that it does the same from the command line:

```sh
./simulate.sh docs/test/foo.txt
```

With `test-ocr.png`, this gives:

```json
{
  "docs" : [
    {
      "doc" : {
        "_index" : "_index",
        "_type" : "_doc",
        "_id" : "_id",
        "_source" : {
          "attachment" : {
            "content_type" : "image/png",
            "content_length" : 0
          }
        },
        "_ingest" : {
          "timestamp" : "2021-04-01T10:15:07.943546592Z"
        }
      }
    }
  ]
}
```

Note that the content which is sent over the wire is very big... For a very little extracted.

Nothing has been extracted although we do have an image.

```sh
open docs/test/test-ocr.png
```

Imagine now a much bigger file, like a small video of hundred of mega bytes.
Nothing will be extracted but to know that we need to send mega bytes of data over HTTP.

It would consume a lot of memory on the node:

* to read the http content as a json
* to extract from the json the base64 string
* to send it to Tika

Show this huge content with:

```sh
./simulate.sh docs/test/big-notice.pdf
```

Instead, let's parse the data from the source and send to elasticsearch only the meaningful information.

## FSCrawler

Enter into FSCrawler dir:

```sh
cd fscrawler-*
```

Create a new empty job:

```sh
bin/fscrawler --debug --config_dir jobs test
```

Explain the `jobs/test/_settings.yaml` file.

Just keep the minimal settings:

```yml
---
name: "test"
fs:
  url: "/tmp/es"
```

Run it again. This will fail as no Elasticsearch node is running.

```sh
bin/fscrawler --debug --config_dir jobs test
```

Show one of the existing jobs like:

```sh
cat jobs/demo-all-docs/_settings.yaml
```

```sh
bin/fscrawler --debug --config_dir jobs
```

Choose job `demo-few-docs` and `demo-all-docs`.

## FSCrawler with Workplace Search

You can show the existing sources and how to add a new supported one.

You can manually show how to configure a new Custom Source.

```sh
bin/fscrawler --debug --config_dir jobs
```

Choose job `demo-wp-all-docs`.

You can look at the interface and show:

* The FSCrawler Custom Source API overview
* The Content
* The Schema that has been automatically generated by FSCrawler
* The Display Settings that have been automatically generated by FSCrawler

Go to the Search Application.

Search for `David`.
Use the faceted navigation feature by clicking on Github (if you have this source). That you can navigate in the result.

Choose "Local files for demo-wp-all-docs".
Scroll to the `test-ocr.pdf` file.
You can click on "View on Local files for demo-wp-all-docs" button to open the document. It is served by a Python local webserver which is exposing the same directory we just crawled.

You can also try those searches:

* `documents updated by David`
* `images` then choose the ones in Local files and then the `png` one.
* `david updated recently`
